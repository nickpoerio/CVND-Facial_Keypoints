{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Convolutional Neural Network\n",
    "\n",
    "After you've looked at the data you're working with and, in this case, know the shapes of the images and of the keypoints, you are ready to define a convolutional neural network that can *learn* from this data.\n",
    "\n",
    "In this notebook and in `models.py`, you will:\n",
    "1. Define a CNN with images as input and keypoints as output\n",
    "2. Construct the transformed FaceKeypointsDataset, just as before\n",
    "3. Train the CNN on the training data, tracking loss\n",
    "4. See how the trained model performs on test data\n",
    "5. If necessary, modify the CNN structure and model hyperparameters, so that it performs *well* **\\***\n",
    "\n",
    "**\\*** What does *well* mean?\n",
    "\n",
    "\"Well\" means that the model's loss decreases during training **and**, when applied to test image data, the model produces keypoints that closely match the true keypoints of each face. And you'll see examples of this later in the notebook.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture\n",
    "\n",
    "Recall that CNN's are defined by a few types of layers:\n",
    "* Convolutional layers\n",
    "* Maxpooling layers\n",
    "* Fully-connected layers\n",
    "\n",
    "You are required to use the above layers and encouraged to add multiple convolutional layers and things like dropout layers that may prevent overfitting. You are also encouraged to look at literature on keypoint detection, such as [this paper](https://arxiv.org/pdf/1710.00977.pdf), to help you determine the structure of your network.\n",
    "\n",
    "\n",
    "### TODO: Define your model in the provided file `models.py` file\n",
    "\n",
    "This file is mostly empty but contains the expected name and some TODO's for creating your model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Neural Nets\n",
    "\n",
    "To define a neural network in PyTorch, you define the layers of a model in the function `__init__` and define the feedforward behavior of a network that employs those initialized layers in the function `forward`, which takes in an input image tensor, `x`. The structure of this Net class is shown below and left for you to fill in.\n",
    "\n",
    "Note: During training, PyTorch will be able to perform backpropagation by keeping track of the network's feedforward behavior and using autograd to calculate the update to the weights in the network.\n",
    "\n",
    "#### Define the Layers in ` __init__`\n",
    "As a reminder, a conv/pool layer may be defined like this (in `__init__`):\n",
    "```\n",
    "# 1 input image channel (for grayscale images), 32 output channels/feature maps, 3x3 square convolution kernel\n",
    "self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "\n",
    "# maxpool that uses a square window of kernel_size=2, stride=2\n",
    "self.pool = nn.MaxPool2d(2, 2)      \n",
    "```\n",
    "\n",
    "#### Refer to Layers in `forward`\n",
    "Then referred to in the `forward` function like this, in which the conv1 layer has a ReLu activation applied to it before maxpooling is applied:\n",
    "```\n",
    "x = self.pool(F.relu(self.conv1(x)))\n",
    "```\n",
    "\n",
    "Best practice is to place any layers whose weights will change during the training process in `__init__` and refer to them in the `forward` function; any layers or functions that always behave in the same way, such as a pre-defined activation function, should appear *only* in the `forward` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why models.py\n",
    "\n",
    "You are tasked with defining the network in the `models.py` file so that any models you define can be saved and loaded by name in different notebooks in this project directory. For example, by defining a CNN class called `Net` in `models.py`, you can then create that same architecture in this and other notebooks by simply importing the class and instantiating a model:\n",
    "```\n",
    "    from models import Net\n",
    "    net = Net()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data if you need to; if you have already loaded the data, you may comment this cell out\n",
    "# -- DO NOT CHANGE THIS CELL -- #\n",
    "#!mkdir data\n",
    "#!wget -P /data/ https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5aea1b91_train-test-data/train-test-data.zip\n",
    "#!unzip -n /data/train-test-data.zip -d /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">**Note:** Workspaces automatically close connections after 30 minutes of inactivity (including inactivity while training!). Use the code snippet below to keep your workspace alive during training. (The active_session context manager is imported below.)\n",
    "</div>\n",
    "```\n",
    "from workspace_utils import active_session\n",
    "\n",
    "with active_session():\n",
    "    train_model(num_epochs)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the usual resources\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# import utilities to keep workspaces alive during model training\n",
    "#from workspace_utils import active_session\n",
    "\n",
    "# watch for any changes in model.py, if it changes, re-load it automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(128, 192, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv5): Conv2d(192, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.15, inplace=False)\n",
      "  (dropout3): Dropout(p=0.2, inplace=False)\n",
      "  (dropout4): Dropout(p=0.25, inplace=False)\n",
      "  (dropout5): Dropout(p=0.3, inplace=False)\n",
      "  (dropout6): Dropout(p=0.35, inplace=False)\n",
      "  (dropout7): Dropout(p=0.4, inplace=False)\n",
      "  (dense1): Linear(in_features=11520, out_features=1000, bias=True)\n",
      "  (dense2): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (dense3): Linear(in_features=1000, out_features=136, bias=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## TODO: Define the Net in models.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## TODO: Once you've define the network, you can instantiate it\n",
    "# one example conv layer has been provided for you\n",
    "from models import Net\n",
    "\n",
    "net = Net().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Transform the dataset \n",
    "\n",
    "To prepare for training, create a transformed dataset of images and keypoints.\n",
    "\n",
    "### TODO: Define a data transform\n",
    "\n",
    "In PyTorch, a convolutional neural network expects a torch image of a consistent size as input. For efficient training, and so your model's loss does not blow up during training, it is also suggested that you normalize the input images and keypoints. The necessary transforms have been defined in `data_load.py` and you **do not** need to modify these; take a look at this file (you'll see the same transforms that were defined and applied in Notebook 1).\n",
    "\n",
    "To define the data transform below, use a [composition](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#compose-transforms) of:\n",
    "1. Rescaling and/or cropping the data, such that you are left with a square image (the suggested size is 224x224px)\n",
    "2. Normalizing the images and keypoints; turning each RGB image into a grayscale image with a color range of [0, 1] and transforming the given keypoints into a range of [-1, 1]\n",
    "3. Turning these images and keypoints into Tensors\n",
    "\n",
    "These transformations have been defined in `data_load.py`, but it's up to you to call them and create a `data_transform` below. **This transform will be applied to the training data and, later, the test data**. It will change how you go about displaying these images and keypoints, but these steps are essential for efficient training.\n",
    "\n",
    "As a note, should you want to perform data augmentation (which is optional in this project), and randomly rotate or shift these images, a square image size will be useful; rotating a 224x224 image by 90 degrees will result in the same shape of output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# the dataset we created in Notebook 1 is copied in the helper file `data_load.py`\n",
    "from data_load import FacialKeypointsDataset\n",
    "# the transforms we defined in Notebook 1 are in the helper file `data_load.py`\n",
    "from data_load import Rescale, RandomCrop, Normalize, ToTensor\n",
    "\n",
    "\n",
    "## TODO: define the data_transform using transforms.Compose([all tx's, . , .])\n",
    "# order matters! i.e. rescaling should come before a smaller crop\n",
    "data_transform = transforms.Compose([Rescale(250),RandomCrop(224),Normalize(),ToTensor()])\n",
    "\n",
    "# testing that you've defined a transform\n",
    "assert(data_transform is not None), 'Define a data_transform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  3462\n",
      "0 torch.Size([1, 224, 224]) torch.Size([68, 2])\n",
      "1 torch.Size([1, 224, 224]) torch.Size([68, 2])\n",
      "2 torch.Size([1, 224, 224]) torch.Size([68, 2])\n",
      "3 torch.Size([1, 224, 224]) torch.Size([68, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leomarcello/Documents/nicola/CVND-Facial_Keypoints/data_load.py:39: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  key_pts = self.key_pts_frame.iloc[idx, 1:].as_matrix()\n"
     ]
    }
   ],
   "source": [
    "# create the transformed dataset\n",
    "transformed_dataset = FacialKeypointsDataset(csv_file='data/training_frames_keypoints.csv',\n",
    "                                             root_dir='data/training/',\n",
    "                                             transform=data_transform)\n",
    "\n",
    "\n",
    "print('Number of images: ', len(transformed_dataset))\n",
    "\n",
    "# iterate through the transformed dataset and print some stats about the first few samples\n",
    "for i in range(4):\n",
    "    sample = transformed_dataset[i]\n",
    "    print(i, sample['image'].size(), sample['keypoints'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching and loading data\n",
    "\n",
    "Next, having defined the transformed dataset, we can use PyTorch's DataLoader class to load the training data in batches of whatever size as well as to shuffle the data for training the model. You can read more about the parameters of the DataLoader, in [this documentation](http://pytorch.org/docs/master/data.html).\n",
    "\n",
    "#### Batch size\n",
    "Decide on a good batch size for training your model. Try both small and large batch sizes and note how the loss decreases as the model trains. Too large a batch size may cause your model to crash and/or run out of memory while training.\n",
    "\n",
    "**Note for Windows users**: Please change the `num_workers` to 0 or you may face some issues with your DataLoader failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data in batches\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(transformed_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, \n",
    "                          num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before training\n",
    "\n",
    "Take a look at how this model performs before it trains. You should see that the keypoints it predicts start off in one spot and don't match the keypoints on a face at all! It's interesting to visualize this behavior so that you can compare it to the model after training and see how the model has improved.\n",
    "\n",
    "#### Load in the test dataset\n",
    "\n",
    "The test dataset is one that this model has *not* seen before, meaning it has not trained with these images. We'll load in this test data and before and after training, see how your model performs on this set!\n",
    "\n",
    "To visualize this test data, we have to go through some un-transformation steps to turn our images into python images from tensors and to turn our keypoints back into a recognizable range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the test data, using the dataset class\n",
    "# AND apply the data_transform you defined above\n",
    "\n",
    "# create the test dataset\n",
    "test_dataset = FacialKeypointsDataset(csv_file='data/test_frames_keypoints.csv',\n",
    "                                             root_dir='data/test/',\n",
    "                                             transform=data_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data in batches\n",
    "batch_size = 16\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, \n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the model on a test sample\n",
    "\n",
    "To test the model on a test sample of data, you have to follow these steps:\n",
    "1. Extract the image and ground truth keypoints from a sample\n",
    "2. Wrap the image in a Variable, so that the net can process it as input and track how it changes as the image moves through the network.\n",
    "3. Make sure the image is a FloatTensor, which the model expects.\n",
    "4. Forward pass the image through the net to get the predicted, output keypoints.\n",
    "\n",
    "This function test how the network performs on the first batch of test data. It returns the images, the transformed images, the predicted keypoints (produced by the model), and the ground truth keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model on a batch of test images\n",
    "\n",
    "def net_sample_output():\n",
    "    \n",
    "    # iterate through the test dataset\n",
    "    for i, sample in enumerate(test_loader):\n",
    "        \n",
    "        # get sample data: images and ground truth keypoints\n",
    "        images = sample['image']\n",
    "        key_pts = sample['keypoints']\n",
    "\n",
    "        # convert images to FloatTensors\n",
    "        images = images.type(torch.FloatTensor)\n",
    "\n",
    "        # forward pass to get net output\n",
    "        output_pts = net(images)\n",
    "        \n",
    "        # reshape to batch_size x 68 x 2 pts\n",
    "        output_pts = output_pts.view(output_pts.size()[0], 68, -1)\n",
    "        \n",
    "        # break after first image is tested\n",
    "        if i == 0:\n",
    "            return images, output_pts, key_pts\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debugging tips\n",
    "\n",
    "If you get a size or dimension error here, make sure that your network outputs the expected number of keypoints! Or if you get a Tensor type error, look into changing the above code that casts the data into float types: `images = images.type(torch.FloatTensor)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 224, 224])\n",
      "torch.Size([16, 68, 2])\n",
      "torch.Size([16, 68, 2])\n"
     ]
    }
   ],
   "source": [
    "# call the above function\n",
    "# returns: test images, test predicted keypoints, test ground truth keypoints\n",
    "test_images, test_outputs, gt_pts = net_sample_output()\n",
    "\n",
    "# print out the dimensions of the data to see if they make sense\n",
    "print(test_images.data.size())\n",
    "print(test_outputs.data.size())\n",
    "print(gt_pts.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the predicted keypoints\n",
    "\n",
    "Once we've had the model produce some predicted output keypoints, we can visualize these points in a way that's similar to how we've displayed this data before, only this time, we have to \"un-transform\" the image/keypoint data to display it.\n",
    "\n",
    "Note that I've defined a *new* function, `show_all_keypoints` that displays a grayscale image, its predicted keypoints and its ground truth keypoints (if provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_keypoints(image, predicted_key_pts, gt_pts=None):\n",
    "    \"\"\"Show image with predicted keypoints\"\"\"\n",
    "    # image is grayscale\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.scatter(predicted_key_pts[:, 0], predicted_key_pts[:, 1], s=20, marker='.', c='m')\n",
    "    # plot ground truth points as green pts\n",
    "    if gt_pts is not None:\n",
    "        plt.scatter(gt_pts[:, 0], gt_pts[:, 1], s=20, marker='.', c='g')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Un-transformation\n",
    "\n",
    "Next, you'll see a helper function. `visualize_output` that takes in a batch of images, predicted keypoints, and ground truth keypoints and displays a set of those images and their true/predicted keypoints.\n",
    "\n",
    "This function's main role is to take batches of image and keypoint data (the input and output of your CNN), and transform them into numpy images and un-normalized keypoints (x, y) for normal display. The un-transformation process turns keypoints and images into numpy arrays from Tensors *and* it undoes the keypoint normalization done in the Normalize() transform; it's assumed that you applied these transformations when you loaded your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAAA+CAYAAABnVDalAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAFpklEQVR4nO2bMW8cRRTHf3txQOBISIizQcQSpZFAormTHCfSninooXBKhPgMFDQhRbooX4OkgCZt5Fs5Nkh2Q4HCB8BCsh0ipUikIOeGYma8c3Mzu7N3s3tn2H9z3rvZ2Zn/vHnv/96sEyEELYrRmfcALgJakgLQkhSAlqQAtCQFoCUpAEuxO8ySrAN0gZNUpP8JfRHVkhRBO8ARMFTXFx6xLakLbKp+N9X1sathqMUtgmXGXukTYB84U58nrkahFrcolhn1oWqlt4CrwKBg5V0WN0u7WhHdcaciHeHZYga0xW1SYHFWu8OCdpVRZRsn80pw7UH6Bp0l2SVgF+gjCdtSCzHrs3fIF6mwz6gkTetkXYNWP+ntdYS0+jPgairSMkste95qlT6j+aSqTjZLsk6WZKuq3ceM+54Vo68HBASDiggKMBrRLKnK6liW8wJYNj73gW2rrzVAEFEGVLH6mI57whkXDMSMWu+o75aBz4An6trs6zi2RgoMMEDE7WaHfyDBv/00oXriQl0/dfXlIsjYrknZ2Kq0daFOcebVOGrSN4HX6qvXwGUMQlORjlKROi2oiv+LIUhrc9zAKcXO8dj4/RAZ4icI9VhBFZE5syCNaUmuwWwhne623djaUteRRI0RWmAFdnSiYCtVimQuxCRpYuAK9/GYunKep0giesABUtjpLea0AoPgNdXOu5UqpEpe1CEmV5DE6FSihyELkKR0jU/wSAdlHUPyKLeNESmrisJpETvBHSEjlV59bR3aurTVHAHPKBeLCdLB+yxm5q0UgjqKbgnj/uUGuSyw9ZHeRpeNbhKjrx3gT+Anirfd1FspBMEklWkNY1J/ARvAJf2bEcrNlX8OnN1bu3c4+G7Qf8QjO/qYhPZxOHbVt1cqxEIQSSoTf0yx1tCT0iQmuPWRXvl3B98Ovn74zcMN3mLpzq073P3orkmASegBMgLWajE+lDpuRcge0jrA4yBVu2fkaYZAlji0+p5IT5LbyQvg7fPWgpfitlg20pmnyMXp4SlplOVgMcq/IZbUVYPUOMDtILvI/AskkZ9Snp5cOf8rATpcUVa7p9rvkkfHCSFo6ag928JjlX9DbjLN/lfgumdF7EjzRLXzKl5xSwj+4Qte8RJBZ/jDMEESs6Hab+DxRQpm39eYJCpK+be0CqCqhluUmGxBO2+pNkuyzpDh98AbSD10E+mkNRLgK2DkefYJ0rKvqese4yc0oWXiQjRSvi0ozU6IQeBn8kkL4IMigWj4TO23BtYzZvZJcy3fOhS19mHeScd4blXErEya1cZDpO8aWb/7Cv3r5D5sIQ4kTcQk6X2kkNQ66RfgRirSke90ouzUwiDwD+ZIWsy0xB58nzyarOCOMt7oowj6G/gdeEX1A4apqpAuxC6V7KJl4Xid+z4yTRGM66yiBHWdXJguERDG6zoWr6PG/SFKSFo6Sa9sn3wC9mon2hKQBwLP1fdnhGX6tRyL11Hj/hH4jZwI01pgfAL2pMzztiHwHvAJ8CZheVstpZPYEsDWPfq8TBfYHjAe7iGXAAfAl1QoormiYB2RMTZJtu7B+Ns8ujYnpSsMPcc9Xsupep4/C6IrbmMlIcAqLOkgkD7Nl4aY9zVSuoUaX71RVhWSN9lEjAIna+dlp4q46Fqq1rSkQGWff6++GktNQidp9KNr57VsvWiWZPuILMn0QF3FOXNCn1td6W1XCsNqVwl8V3MaxNxuoS+V2up7PfA+wBu9opREfKjzcHJioA71vY8UjUHaxqeo6z41abRU4otIxqFm4TtITUY0E9EPJ0uOd4qszXscHnh/bWj8xVKPSq76llyjZZO5vX1rwlWhDKxsNkLYQpAEU5V+G0tLoivuaVHlHUaF4P9jmRVz+V+NSGjMiS/MdpsG/zuftMi4yNutMbQkBaAlKQAtSQFoSQrAv5Cx8cKwS+MfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAAA3CAYAAABAW2dtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAFMklEQVRoge2aTYscRRjHf7U7FxFUSHYCIbl4Wi8SRBc0ce3M5pYPsLL33DzObUEQPOkH8LIXDxNWUDx4FbeT7EIwh+wh2aseRJ0JERJJBJnZ9vBUpWsr1T3VPdWzLcwfhp7pqa6Xfz3v1SrLMhYox9JpT+D/gAVJAViQFIAFSQFYkBSABUkB6Jz2BAxSlS4BK8AoyZJG4hI9xk/AZeAA6CVZcjztuVZIkjX534A9/bsJrCAEdfR1JeShVpBEzcnXwAiRoLG+jkIeagtJ3smnKl1KVXouVamKMYhW4x5wAbgaqtbqNNISn/1x73nsxwZwlgZtVhHmTlKo8UxVeg6xUR1Ewn4G1sqeaQqNezeP1HSBdUDpaxf40/OoUcHLwD2EINtmDZueu0GjNqnAa7mi6xVlx34YqatkcGPZtKYNt+u1uvr+bWTBtykw0h4J7AEXgc2QgWOGFVFIKtkx12vtIpMGWfBVx0ibBS3jX+Cu514RooUVM5NUtmOOBHyC2KCOvmJ5KXdBV5y2XU+baYuuFRP5EEOSQia/C9xHjLWBbYseId5rDDwDfvS0rRRL1Y2JfIjh3Wwv5Nsxm8QMOAb2sRaJSOIacAhcstpOdJ8GPUpiqVSlJ0ID/d3rBavkijNLUsCOGRIzRDrMmGbnbRIvIe5+DNxF1BS0KuvxhiVqGmR3qhr1KIY7yZJjZ/L2fxnwMSIVIOTYC3LV6CPgAfAe8APlJNS1O5XInVfuNiSXpgxrQY5x3wIek6vcu8BzCkgokuKA+KgSuUFpSYxaj+6jizbCdj+W+K/vqB016A/Y/nKba1xDL2QDuBMydoW0J3hNU0mqW6iqApOn7aidzuDTwYv7W59vjW9MbjwDXrXHnkK4m/NdSLJkphQmRN1mCsoCU4MRcDDoD2SxuuXN/s1/EYJejG1t2u/AH1iG1zLAUeIjgxCSagdlZV7EJs/YlvO/nH8byCOop7zpjA35piksJ+CMBVZEHzrfIkwlacagzGT8duTsJS/JkuPBt4OH17++fot/GG9/tn1r76u9EblRR7f/Br8T6HJS4rNYdaegYLIsKLNRYAyVcwW/Cg8B1f+1T/+Lvmm3hBTaMqe9Ic1E4gqJ6pdxvGcMRAsBSsoi9m5+p5NX8KuwS94d3d/35GnLAUKoTYKtghNgM2b1Mmac5C7wLWQhd602a+QLh5fthm3/DpGAsgO8rz/3EPVTWBuC5H5RjbWNaOVb7b32EIJst72B1I3WyEuwhe5ZS5pp/xx4zfp7jNhGcNw8QlQj53bRyrc64ewhEnRILlFngQ/RCyAnsmjHVxCpUQhBJufLEEkyZJxIqjUxjZR0o6Yl2sAfcVL03R2e5inde0r3dYSo31804ObL0MhpieXlHlExWtdquw98YN2eIJ7Lxhgh6aWoOzYaPVKqmyJokvcRyXmAJLwGfwOvkAeXjaVLBk1XAWpF63qxVxC1fAd4ov96Cryh728yn6Px5g8nI1UQloFV4Mgqh9je9IAA21R3LqdyzF0VPpL0/eBFz1LNaD1JmqDHwOuI2p1JsmRS/pS3n9ollNa8xAWFhblVhCD0dRV4OKUPn3RNO7AoRFtevbHVwa0THZEb7if697Q+ys4AK1cz2iRJdpKK/r6SZMkwVekZPDappA/vixWh1QwXbSLJqMO6/m0fFkwoUTFPH5VVqgytMdyWPYIZougmXlBtBUnzOGyYBW0x3PN6sbQW2kJStDdAmkAr1A3m87J7XbSGpDajLerWaixICsCCpAAsSArAgqQA/Ad0AhyzkdBX/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAABACAYAAABWfFoUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAFMklEQVR4nO2bvY8cNRjGf949USQRV2UTRUmfdGkAkdOh2aWgp8kKUaVOS4mAfyD/AGmRQEI0aUPOfCyRIoqjuY7qAohbKVKQOCRyOlPYvvV6PXvzYc/Monmk1ezMeT324/d9/fi1Tyil6LEeg7YbsAnoSSqAnqQC6EkqgJ6kAuhJKoCtJl8mhRwAl4GjTGUboz0asyRD0BPgObBn7jcCTTb0MrCDtt4dc78RaNLdjoAZmqCZuS+NNlxWNLkskUIOgZvAQZUOOi5riZ5kKjuN28pVNB2TvgX2qR6Tgi4rhRxIIa9IIUWs9rrYtJhkXfbEXOdSyKtUmBDKENskSX4HS8ck46IT4Lq5PgEOgXcoQX7ZmbYxkrwOjqsG3Uxlp5nK/mTZMqEc+aWsuumYFHNWci3ze+A2xckvZdWNkJRCSDqWecM8KjwhlLXqRiSAFPIKmqAt9OhdNy7T6botmnK32kG7pbqBBsVkSqWcWoU3qrhdOB2bAyPzWNHBDEErK3EvkL8Afgf+MJ/OZQjaasyIhQDcBqzqFXQwQ9Bo0s2B704KTZDCC76eW7rXFbdMFZvasqQjtAA8Ab4DnrJMnICgW7rXJbdMmdSLaknuSKI7mjeqApiyIOY5C5ez7uYvPbbN37edcrekkDbtElpqRNFL0dj2R5KcUXXKHQJfod1nhibMdzdXA730rscsq+xkeikKSaaRt1geybwFZGjEJ8A187nrte8+eumxb579Yr6/7tYRawEdQm1387KFfwMX0SMJ4VTtHHgGvA0M0dY0MWXOso5SyA+AA7R7/QVcMO294zXhH1MnJksZdUkCEcRkYO10G925lZjkETo0ZU7Qo49TjwJOTRmLn03dM+A1NMkWPwG7qVK5MdzNjwUHmcqUzft4Zp+XA5qbe1sPD8XD4fijMY95DDoOvYVxJWCXhfsBvElCbRVlWeLrkzy9YlKleyzc8C6aIDe5P31w48E3j+49WljKb3yoPldfOO+y5Y/RbjjDxKESM2xhRF+7nbejESD0KnpZYsXktfHH418ZcOHsieJYfaYumt8H3dshyH03ee0ogxSKe61eyQmuYuk65BKK0zMVNeCSU9bfv3O3p/x3k9eOMoiquB09VEavWH1kv7P36d6IV7zHvxwDA/XJwtwDmwEjZ8fDj49RdFM0dwuY+hTwA3fod36cglU5seIqeW7d6ZhUJ43qdAynDotgXSaWHVZ5X1nEjElz9Ohvm+t8ffEFbJwyVmXjjWtJS65iSP0SraNWMgdOmSgZgU5YklePnxrxZcUcvQTaz3tX7DMDtSzJ8//ap0YC8cR97i99ci2N1VluJIWsnBqubEmh0XIaWKgxAVLyNM6URfyxOAHeBX4IJN/yJoNKVlWHpFrulTMbuoEYt270QnjHew75gnXEcr6qchioo5Pq5m98lxDoZQZoV/LrnqDVtcUWgVTMmnxVZb1UmaQI+Ruf5K/ROSLQsWbq1m0s5cD5zUvCHc/LV1XOM7W27wZr9dFTYCfUobzZz/m7H49qJ+Da2i0BcvXRM+D9UPmA9gnFFjd/HmXXpBObgN4JkVecvxvyY2g3JBCPohwPjH50riqMVSmK5cbvECYqyTHoc0lq+JD6uhnzCO2KFm+wSkKSHZMiHW7skPq6GdN830UH9SAJqXZMzp3dUswWddDZw+5NNqyL/6TTqk7yUWf1npLcTkgAB5XiX+rJpWskVZ2dkk4unSKpxuyU9HBpp2JSHaSMSf8bklKiU+7WVfQkFUBPUgH0JBVAT1IB/AcKeWX6IoPwdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAAA2CAYAAACLB7TIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAFAklEQVRoge2bT28bRRjGf5uYSxBUonFCoZW4llMurQALtE7zEZCIlGsPfAEOyFzoNb1y6r1qP0TxKm0oSjj0AG3vVKJ1QkWRCBJKMhzeGc/sZm2Pd8e7sbTPxfGfdzzz9P37jBsppWgwHgt1b2Ae0JDkgYYkDzQkeaAhyQMNSR5o1b2BLJIoWQDawCBW8bnoT6LQfVKZQ2rbH4EOsAusxyo+DbrBAggabs4hXwB9/XwatBGCWvqxHXJ/RRE6J5U95ADxoGP9OAi6u4IITVKpQ+rwXAcuA90mJ80RgpBUBzFVfmdpknIq0g1gmRluvuoqGIKkVaSatZBctAdcZ4abz/nONeApEDED7wqRuN1kvY8Q5F3dkihZSKLk/SRKVpMoiQp85z/AE6BPufZjJILnJGSzJgzGVignbL7QL+0g4doGFGM8QtteRQgyHoXz9+VYxa/KnUxQa3XLhA3YcP1UP99hTMhqz3P/UXD+/spnDz4ITpIPcjzPeNLPwDXSpI31iMxaEbAC3CNgUq98wM1Upj2EoGF4kSZtH6chzfNSTYAhUSVRojjb9ZcKuzqkkhWEhBbwGfAQONDvRcAm8CHwGPGqvk7uvnNh8NGm8nDTeegPhBCwVfEaUqnedp6bJHwFuEg6SY8Mw9CNphdJRb7UsTlwbZ1ka0LqMbZtMMj2WyChY0icWDlDYiJJRbrbjI17sPVYxaf6/RXSeagDHAHv6GV2kNBT5DSOmvCy2pWXrU9OKiJ/uDYXsraxik9jFb+MVfxKb/AG4jlL2j7SnzfYRchaBH4AojLa1bS2PgsXSYSuzRsP22XSIWc+fw85yFvACZa8NuW0q6lsJ5JUROPJ2LznYTsk9fZHt3e633b/637d3XQOch1J5i7ZZarYVLZ1VLczucC4/53FO527390dJvCtW1vqproJuvMeYTfznFQpSaOKgBlPut90WywhQaWAI+hv98+U+1GVc1b7rqSZ1M3gKlLR8nLBANjtbffkoPq4ve0eSHU8cNfCJt3XzGDqzyLYwoYIV+4wMgj2UM+RCoV+vJ9EyYL2gs0NNk62bm3Bvxz3vu8db7AB0j5cddYdWzlngVBSySKSN4Zim37LhNYitsN2oYAPYhW/1J7wGjn4G+A3vV6qz9I2pq+qpLkMJd8+wsobx0g1A9sEKqSEm0OZ5KyAS8AhcugHmc8/QZrH1ChSdU4KoQK0kTnLYA9bUndJ6zvmUPed1w+BPxEPOkEIivTe1rBz3LBUZyb/IMLaOITwJFf42gc6k0p0RgP6GPjVWdKQBPAT8PmkNc79bQmU7lci4C/gXf2SCbWRhCMkzs9tybTIDLcm/A4RVXKNHOk1p7/aBH5nBnp2HipVJnOE/7+x1ekTRt/XZWctRTrfTS2sTeP9Vcu3RpU0OeeCfuwAy2O8wcxaruBfeEzJemYSJWPDtWr5NrvxEzyGTGdgvqJfeoEc8iAnJH068LAqQGAMECXSQCF5qItoRJMuKC8y+nDTHHwuVIBHSA4CeyH5AHuD8iVwipV8Ryqd2IoH01+Mnj8VwEDPc251MiS5OVKRlkiyEu4zMm2AtgveO9VFktuAGs84wvZKBmbEOXN9jhQBl7iZtQG1/ETZScRr2FluCfgFm9xNmR/E+epoZT8drMWTDHLu8tcRD4EJP5jQ9pWMJrWSBPPx88HaSZoHNP9twgMNSR5oSPJAQ5IHGpI88D/K3CACuInZTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAAA1CAYAAAANk8ZmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAFIElEQVRoge2az24bVRTGfxNbSLRSI5XEQaFZUySCGiSiIqdlSPwOLQt23bEEsTF7EH2CKhW7Wu1L4FFoQpUsyKKkrAlCxBagFIUFcjoszr3x8c3YM2N7ZhzJn2RZnn/3zHfP+c4599oLw5ApBmOmaAMuAqYkJcCUpASYkpQAU5ISYEpSApSLNsAi8IIZYB5o+aE/UXXJRHiSIeh74DegaX5PDCbFk+aBKmJP1fw+SvOALD1xUmasBWwDHfPdSnNz1p44ESSZmV8HrgEfD+EJUZ44NkxKuOGH/itShpiC9cQqQ3hiHLw8Glzj/hUgJKPslaUmZU6S0ovb5tAWsO6H/qtJTvsaeWiS1QvPfKrA/KSnfY08NMnqhfUkqxkVRkz7Ghe6BFCZaxF4C/jMnBop7Wtk7ZWJNGkcsxR4QQn4E5gFjoE584kV87jxAy9YQAgqI6TfAA7G5VGxjI9xlq4jBGG+d4FD4AmiVaOMr73yBNgf0dYeJNGkgS2DmuU2g73tAPGgWeAUWDHHrZD3u7/v+NrDkJB+ByFoLDpnkYTpvtrhzPJfDJht8+JvAL4z7r/mdz9viRzfhO9Te4+59gXiSZjvdoL3i0UsSTEtg57lWWLaAj/0T4FfnMNXgF+Bj8z9t5HMZydhAbirxzfHt4APnTHngcvmuZf72ZEWiUqAAS2DbgdOjGFxmaoFPENesJ8dYb8iNPACDwmrVXX9nhpz7O3JyBV3Ck3S97wJ/E5XsK3gXjG/txDvOaRLYAd4H3gAfEB3UnaBNTtmVCYcNTvn0ru5MEY/RV52G7iDEHYIlDdLm53Gl43/6l/X92rUrCe9REjUxMameuWR1rvWTWQkRu4kOUbvArdMH+cBzc3SZrXxVaOMh1RQh3za/K65D/xEb1j+CKwR0zhH1FDX/NBPlfFyWSpxUrUW+1V60/9G4/PGyx67KjxANKdkjoQIQbdwNCvwgg2kQNWEjbyMknlb4pQJ24h26ZTeVue36t/WXwOECqD+Tf11hAjP3LOMeFBU4/wMp4zol50DL5gJvGDBePBAZBpuxlBd4AHsIC89h8zqAl2B7gB7D72HNx998cir369To6YfeQxc1eFJ15P+oSv8A8MqrU5l5knKkH2kYLRYBebMC3jAYySUQmPw2r3w3mLzfnOnRs2dwbPax2mcl4FL6jpdEkQh1XJvluGmDbmEGO5W7TpkToFPEM87QjRnEdEfS1YJeBJ4QckIMubcAd0Q3gGqMak+1QpEliS5htzkfNWMc80L4DnwN+D5of8Hoj/vmWus9vyA0woBG+b5a3G1UNqNhzw06VwRF6EJd5G+7rm6/V0/9H8211v9qSKlwAq9mXmo1J4URdVJWsw7yIy2EA+y601v01s1lxAP0tW2boWG2YpKhFzXlR0xP0FpglolWDbnz1K5ue86QlAZIeYGcBVYQir2zJD34nsFSdllJF2voDzArBK06c08FaKJPTDPfEzGmwl5k+SGQztCq6BXzEO6pFkPssRmunNrkfcObgvp8K1gt00qbyGZS4v5Et02RrcVB+NsOZKgKOG2SytuhtOV9xISSvr8kSvOwyzVpEUhSyUQ3Z0jmwKWlDvu+XG1GWlR5K5pVNV7VuD1Od8PmWpTYSSpqtemcA8VLimr4rFtdEahsHCDc2GiC8PU4ZLlNnfR/09yd1tgyP2yEf/fNBBF/5NDh8kxGYXLqCg03CCfFD4qCifpIqDocLsQmJKUAFOSEmBKUgJMSUqA/wERKsIC3nCsiwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAAA4CAYAAACxDdW4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAFEUlEQVR4nO2aXWscVRjHf5PuTVuMqE22vn0AvYqggmwMszF+AW9UeutHEG/2Qqr0yk8g9NJChRb8ALUd0iaBtKAg+AXqS7OrQitNoaSOF885nWdPzrwtZzbTMH8oO9nuzsz5z//5Py9nozRN6VCMhaO+gWcBHUkV0JFUAR1JFdCRVAEdSRXQC3myJEoWgCVgHKfxsaktginJEHQd+A24Yf4+FohCFZNJlPQRgnrAAfAaMOEYKCvk0x4DWwhBWwhBx0JZwZQE054ELOMoK07jvVDnn6cyg5KkkURJBNwABoiyhnZhsyxWeZ4933qcxv81ce8uGgsBs/h1xJtcgmYJwyWEoJ55XSr7QhIlC0mU9M0DmxlBSwAX5km7IeZbbG4YOiG8RaakcdG1XeUlUTKz8holSUMtdgI8BJ43rxPPZ8ZxGqeeEPsAOEO1MK31MIowl4zjhNhN4LT5r9OYsMkJQ3ehZ+I03qvoY262LVReERozbg1PDfUIeA64D7yEqAP8ddY/iOruAy/WCZlQ2bDJ7Ka9BLJMtw8smvcOgJ+BFeRpA6yZ103gE+AuAcuIWRDckww5y8BlVLo2/95ASLF4BLxtjgfAW8BPZOGVUt+sg9dRQT1J+cpdRBFPTdOEya9kPnEHOKm+fhv4G1EaiKn/5VwiN5U32TtWOlGNekMbLTim6dRO75IRtg08Bn4nC8VF4E2q10a166iqKCWp5hPSGWUTp5AEqZ1UhrKEfYQsTD+ECPgWOEEWdkXhFiybuSg1bk9mWkHCJiIzZu+x6wt5nmHevwm8R0ZUCjzBY9ol5wnuSVVI0j3YQ6S2sZlo4Dn2VrZ5vZd632a1f4FTnvMOcwrMxnu40nBTYbGCEGRjfpBznOcFXs8Yfjb8dPjFcHCNaxGiolNIZT3E0/vlnadJVDJuT2baKjjO84JDnhGdjz7kVb7jJL0LX17gYnQxRbLbj4h68VTYud4TqqF1UTlNOka7XvBR7w16pwJP+EF/49Lnl0CymlclqgYLOV0oRa0T2czEYcnXln8SJQujr0e3AbFoYPTNSBO8i6MSMhKuAxNHYe49LRepqo7qZmXblbw93gUmvou7Txrob7Cxdu6rc7BPOjo/urPBRopQtgOslpDgPgz3ni6To6q6qpu5d3N6M5vC32E6A+oMZlsSXUr8wnTKj8zrJk7WKpp0eu4JCkbHvk2Lop4wSIPrXNRCd/I2ZVsCd4FVskWfYNrLntZjs9RBZYRWIVwjFEl5tdSQwxsCuuu3Q7TvkTrJKukBjhqd65WSVfaZOoVnyH03PXnU00VN4C7Ss03J3Hy3j8yWUqbD0g2VuReT8xq6af/SMv8YIbUPXCFrelGfWWea9LPAH2SqeyVO43tN3v9cSNJQqrlKZvSLZJ50ALyOEKD9TBP2JxlJLzc9iDuqXdUrSDPbQ0az2rT3EdXsIWTaudQa4m9jJPvZSUOwbj8PR6EkNxM+QObdWkk2K95CyAQTWmQKm9tObiNKKqlmddG3DbyALH6b6X5sCQlHix2yAtFXcTeG4CSVVbNOD7dqWp17wPtM92OazB2ywdzcun+LJjYnSzcFfTu7nve0Ch8jIVZ5UyAkmgi3UGNUH9m++VLjaMS4Q4xR67YOTWLu2a0O2vIbzFaT1BbM7VclRWiLYvJw5L9jfBZ+tduGG5r77kddtIGkxnZeQ6EVxt12T2oFSW1HG8Kt9ehIqoCOpAroSKqAjqQK+B9HzQi4fkOV9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the output\n",
    "# by default this shows a batch of 10 images\n",
    "def visualize_output(test_images, test_outputs, gt_pts=None, batch_size=16):\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        plt.figure(figsize=(20,10))\n",
    "        ax = plt.subplot(1, batch_size, i+1)\n",
    "\n",
    "        # un-transform the image data\n",
    "        image = test_images[i].data   # get the image from it's Variable wrapper\n",
    "        image = image.numpy()   # convert to numpy array from a Tensor\n",
    "        image = np.transpose(image, (1, 2, 0))   # transpose to go from torch to numpy image\n",
    "\n",
    "        # un-transform the predicted key_pts data\n",
    "        predicted_key_pts = test_outputs[i].data\n",
    "        predicted_key_pts = predicted_key_pts.numpy()\n",
    "        # undo normalization of keypoints  \n",
    "        predicted_key_pts = predicted_key_pts*50.0+100\n",
    "        \n",
    "        # plot ground truth points for comparison, if they exist\n",
    "        ground_truth_pts = None\n",
    "        if gt_pts is not None:\n",
    "            ground_truth_pts = gt_pts[i]         \n",
    "            ground_truth_pts = ground_truth_pts*50.0+100\n",
    "        \n",
    "        # call show_all_keypoints\n",
    "        show_all_keypoints(np.squeeze(image), predicted_key_pts, ground_truth_pts)\n",
    "            \n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "# call it\n",
    "visualize_output(test_images, test_outputs, gt_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "#### Loss function\n",
    "Training a network to predict keypoints is different than training a network to predict a class; instead of outputting a distribution of classes and using cross entropy loss, you may want to choose a loss function that is suited for regression, which directly compares a predicted value and target value. Read about the various kinds of loss functions (like MSE or L1/SmoothL1 loss) in [this documentation](http://pytorch.org/docs/master/_modules/torch/nn/modules/loss.html).\n",
    "\n",
    "### TODO: Define the loss and optimization\n",
    "\n",
    "Next, you'll define how the model will train by deciding on the loss function and optimizer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Define the loss and optimization\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.SmoothL1Loss() #nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Initial Observation\n",
    "\n",
    "Now, you'll train on your batched training data from `train_loader` for a number of epochs. \n",
    "\n",
    "To quickly observe how your model is training and decide on whether or not you should modify it's structure or hyperparameters, you're encouraged to start off with just one or two epochs at first. As you train, note how your the model's loss behaves over time: does it decrease quickly at first and then slow down? Does it take a while to decrease in the first place? What happens if you change the batch size of your training data or modify your loss function? etc. \n",
    "\n",
    "Use these initial observations to make changes to your model and decide on the best architecture before you train for many epochs and create a final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(n_epochs):\n",
    "\n",
    "    # prepare the net for training\n",
    "    net.train()\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        \n",
    "        running_loss = 0.0\n",
    "\n",
    "        # train on batches of data, assumes you already have train_loader\n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            # get the input images and their corresponding labels\n",
    "            images = data['image']\n",
    "            key_pts = data['keypoints']\n",
    "\n",
    "            # flatten pts\n",
    "            key_pts = key_pts.view(key_pts.size(0), -1)\n",
    "\n",
    "            # convert variables to floats for regression loss\n",
    "            key_pts = key_pts.type(torch.FloatTensor).to(device)\n",
    "            images = images.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            output_pts = net(images)\n",
    "\n",
    "            # calculate the loss between predicted and target keypoints\n",
    "            loss = criterion(output_pts, key_pts)\n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # backward pass to calculate the weight gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # print loss statistics\n",
    "            running_loss += loss.item()\n",
    "            if batch_i % 16 == 15:    # print every 10 batches\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss/16))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 16, Avg. Loss: 11.893603146076202\n",
      "Epoch: 1, Batch: 32, Avg. Loss: 11.824322521686554\n",
      "Epoch: 1, Batch: 48, Avg. Loss: 11.719913125038147\n",
      "Epoch: 1, Batch: 64, Avg. Loss: 11.944126427173615\n",
      "Epoch: 1, Batch: 80, Avg. Loss: 11.744899034500122\n",
      "Epoch: 1, Batch: 96, Avg. Loss: 1590983527497739.0\n",
      "Epoch: 1, Batch: 112, Avg. Loss: 11.897872149944305\n",
      "Epoch: 1, Batch: 128, Avg. Loss: 11.819701910018921\n",
      "Epoch: 1, Batch: 144, Avg. Loss: 11.724853754043579\n",
      "Epoch: 1, Batch: 160, Avg. Loss: 11.7818021774292\n",
      "Epoch: 1, Batch: 176, Avg. Loss: 11.82601946592331\n",
      "Epoch: 1, Batch: 192, Avg. Loss: 37726053203979.36\n",
      "Epoch: 1, Batch: 208, Avg. Loss: 11.809492826461792\n",
      "Epoch: 2, Batch: 16, Avg. Loss: 11.934820890426636\n",
      "Epoch: 2, Batch: 32, Avg. Loss: 12.002985775470734\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-159869c4f292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# alive while training your model, not part of pytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#with active_session():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-34d56435b22a>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(n_epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# forward pass to get outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0moutput_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# calculate the loss between predicted and target keypoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/nicola/CVND-Facial_Keypoints/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train your network\n",
    "n_epochs = 20 # start small, and increase when you've decided on your model structure and hyperparams\n",
    "\n",
    "# this is a Workspaces-specific context manager to keep the connection\n",
    "# alive while training your model, not part of pytorch\n",
    "#with active_session():\n",
    "train_net(n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data\n",
    "\n",
    "See how your model performs on previously unseen, test data. We've already loaded and transformed this data, similar to the training data. Next, run your trained model on these images to see what kind of keypoints are produced. You should be able to see if your model is fitting each new face it sees, if the points are distributed randomly, or if the points have actually overfitted the training data and do not generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net_sample_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-992d40565b9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get a sample of test data again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_sample_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net_sample_output' is not defined"
     ]
    }
   ],
   "source": [
    "# get a sample of test data again\n",
    "test_images, test_outputs, gt_pts = net_sample_output()\n",
    "\n",
    "print(test_images.data.size())\n",
    "print(test_outputs.data.size())\n",
    "print(gt_pts.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: visualize your test output\n",
    "# you can use the same function as before, by un-commenting the line below:\n",
    "\n",
    "# visualize_output(test_images, test_outputs, gt_pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've found a good model (or two), save your model so you can load it and use it later!\n",
    "\n",
    "Save your models but please **delete any checkpoints and saved models before you submit your project** otherwise your workspace may be too large to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: change the name to something uniqe for each new model\n",
    "model_dir = 'saved_models/'\n",
    "model_name = 'keypoints_model_1.pt'\n",
    "\n",
    "# after training, save your model parameters in the dir 'saved_models'\n",
    "torch.save(net.state_dict(), model_dir+model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've trained a well-performing model, answer the following questions so that we have some insight into your training and architecture selection process. Answering all questions is required to pass this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: What optimization and loss functions did you choose and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: write your answer here (double click to edit this cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: What kind of network architecture did you start with and how did it change as you tried different architectures? Did you decide to add more convolutional layers or any layers to avoid overfitting the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: How did you decide on the number of epochs and batch_size to train your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Visualization\n",
    "\n",
    "Sometimes, neural networks are thought of as a black box, given some input, they learn to produce some output. CNN's are actually learning to recognize a variety of spatial patterns and you can visualize what each convolutional layer has been trained to recognize by looking at the weights that make up each convolutional kernel and applying those one at a time to a sample image. This technique is called feature visualization and it's useful for understanding the inner workings of a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, you can see how to extract a single filter (by index) from your first convolutional layer. The filter should appear as a grayscale grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights in the first conv layer, \"conv1\"\n",
    "# if necessary, change this to reflect the name of your first conv layer\n",
    "weights1 = net.conv1.weight.data\n",
    "\n",
    "w = weights1.numpy()\n",
    "\n",
    "filter_index = 0\n",
    "\n",
    "print(w[filter_index][0])\n",
    "print(w[filter_index][0].shape)\n",
    "\n",
    "# display the filter weights\n",
    "plt.imshow(w[filter_index][0], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature maps\n",
    "\n",
    "Each CNN has at least one convolutional layer that is composed of stacked filters (also known as convolutional kernels). As a CNN trains, it learns what weights to include in it's convolutional kernels and when these kernels are applied to some input image, they produce a set of **feature maps**. So, feature maps are just sets of filtered images; they are the images produced by applying a convolutional kernel to an input image. These maps show us the features that the different layers of the neural network learn to extract. For example, you might imagine a convolutional kernel that detects the vertical edges of a face or another one that detects the corners of eyes. You can see what kind of features each of these kernels detects by applying them to an image. One such example is shown below; from the way it brings out the lines in an the image, you might characterize this as an edge detection filter.\n",
    "\n",
    "<img src='images/feature_map_ex.png' width=50% height=50%/>\n",
    "\n",
    "\n",
    "Next, choose a test image and filter it with one of the convolutional kernels in your trained CNN; look at the filtered output to get an idea what that particular kernel detects.\n",
    "\n",
    "### TODO: Filter an image to see the effect of a convolutional kernel\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: load in and display any image from the transformed test dataset\n",
    "\n",
    "## TODO: Using cv's filter2D function,\n",
    "## apply a specific set of filter weights (like the one displayed above) to the test image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Choose one filter from your trained CNN and apply it to a test image; what purpose do you think it plays? What kind of feature do you think it detects?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: (does it detect vertical lines or does it blur out noise, etc.) write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Moving on!\n",
    "\n",
    "Now that you've defined and trained your model (and saved the best model), you are ready to move on to the last notebook, which combines a face detector with your saved model to create a facial keypoint detection system that can predict the keypoints on *any* face in an image!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
